{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "%matplotlib inline\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master = pd.read_csv('data/master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dups = master.duplicated('Comment')\n",
    "data = master[-dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data.to_csv('no_dups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88191"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hilary_sim = data[data['Candidate'] == 'Hilary']\n",
    "sanders_sim = data[data['Candidate'] == 'Sanders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biden_sim = data[data['Candidate'] == 'Biden']\n",
    "trump_sim = data[data['Candidate'] == 'Trump']\n",
    "bush_sim = data[data['Candidate'] == 'Bush']\n",
    "carson_sim = data[data['Candidate'] == 'Carson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hilary_sim.to_csv('hilary_sim.csv')\n",
    "sanders_sim.to_csv('sanders_sim.csv')\n",
    "biden_sim.to_csv('biden_sim.csv')\n",
    "trump_sim.to_csv('trump_sim.csv')\n",
    "bush_sim.to_csv('bush_sim.csv')\n",
    "carson_sim.to_csv('carson_sim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    out = text.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    return len(out.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Recommendations'] = data['Recommendations'].apply(lambda x: 0 if x == np.nan else x)\n",
    "x = data['Recommendations'].values\n",
    "np.percentile(x, 95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    54559\n",
       "1     3020\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Rec'] = data['Recommendations'].apply(lambda x: 1 if x > 79 else 0)\n",
    "data['Rec'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hilary     24982\n",
       "Trump      11679\n",
       "Bush        6839\n",
       "Sanders     6459\n",
       "Carson      6372\n",
       "Biden       1248\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Candidate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cols_encode = ['State', 'Candidate']\n",
    "encoders = {}\n",
    "for col in cols_encode:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(data[col])\n",
    "    encoders[col] = le\n",
    "    data[col] = le.transform(data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = data[['EditorPick', 'Sentiment', 'Subjectivity', 'word_count', 'Rec', 'State', 'Candidate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recs = data['Rec'].values\n",
    "# recs = np.nan_to_num(recs)\n",
    "# plt.hist(recs, bins=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = reg.drop('Rec', axis = 1).values\n",
    "X = np.nan_to_num(X)\n",
    "y = reg['Rec'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.530558\n",
      "Precision: 0.820548\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "y_pred = rf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print 'Recall: %f'%(metrics.recall_score(y_test, y_pred))\n",
    "print 'Precision: %f'%(metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.000000\n",
      "Precision: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "svc_linear = SVC()\n",
    "y_pred = svc_linear.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print 'Recall: %f'%(metrics.recall_score(y_test, y_pred))\n",
    "print 'Precision: %f'%(metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "commentOnly = data[['Comment', 'Rec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = commentOnly['Comment']\n",
    "y = commentOnly['Rec'].values\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#type(commentOnly.loc[0,'Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "vectorized_X_train  = vectorizer.fit_transform(X_train)\n",
    "vectorized_X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98263, 64621)\n",
      "(98263,)\n"
     ]
    }
   ],
   "source": [
    "print vectorized_X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reg.drop('Candidate', axis = 1).values\n",
    "X = np.nan_to_num(X)\n",
    "y = reg['Candidate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = dict()\n",
    "for i in range(6):\n",
    "    weights[i] = x[i]/float(x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 8.0904054263967272e-07,\n",
       " 2: 0.0,\n",
       " 3: 8.0904054263967272e-07,\n",
       " 4: 3.2361621705586909e-06,\n",
       " 5: 0.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line statement', (519, 0))\n",
      "An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line statement', (519, 0))\n",
      "An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line statement', (519, 0))\n",
      "An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line statement', (519, 0))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line statement', (519, 0))\n",
      "An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line statement', (519, 0))\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "JoblibIndexError",
     "evalue": "JoblibIndexError\n___________________________________________________________________________\nMultiprocessing exception:\n    ...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    157     pkg_name = mod_name.rpartition('.')[0]\n    158     main_globals = sys.modules[\"__main__\"].__dict__\n    159     if alter_argv:\n    160         sys.argv[0] = fname\n    161     return _run_code(code, main_globals, None,\n--> 162                      \"__main__\", fname, loader, pkg_name)\n        fname = '/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    163 \n    164 def run_module(mod_name, init_globals=None,\n    165                run_name=None, alter_sys=False):\n    166     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x1006fb0b0, file \"/Use...2.7/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/Users/datas...python2.7/site-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x1006fb0b0, file \"/Use...2.7/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/Users/datas...python2.7/site-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    587         \n    588         If a global instance already exists, this reinitializes and starts it\n    589         \"\"\"\n    590         app = cls.instance(**kwargs)\n    591         app.initialize(argv)\n--> 592         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    593 \n    594 #-----------------------------------------------------------------------------\n    595 # utility functions, for convenience\n    596 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    384     def start(self):\n    385         if self.poller is not None:\n    386             self.poller.start()\n    387         self.kernel.start()\n    388         try:\n--> 389             ioloop.IOLoop.instance().start()\n    390         except KeyboardInterrupt:\n    391             pass\n    392 \n    393 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    146             PollIOLoop.configure(ZMQIOLoop)\n    147         return PollIOLoop.instance()\n    148     \n    149     def start(self):\n    150         try:\n--> 151             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    152         except ZMQError as e:\n    153             if e.errno == ETERM:\n    154                 # quietly return on ETERM\n    155                 pass\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    835                 self._events.update(event_pairs)\n    836                 while self._events:\n    837                     fd, events = self._events.popitem()\n    838                     try:\n    839                         fd_obj, handler_func = self._handlers[fd]\n--> 840                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    841                     except (OSError, IOError) as e:\n    842                         if errno_from_exception(e) == errno.EPIPE:\n    843                             # Happens when the client closes the connection\n    844                             pass\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    428             # dispatch events:\n    429             if events & IOLoop.ERROR:\n    430                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    431                 return\n    432             if events & IOLoop.READ:\n--> 433                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    434                 if not self.socket:\n    435                     return\n    436             if events & IOLoop.WRITE:\n    437                 self._handle_send()\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    460                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    461         else:\n    462             if self._recv_callback:\n    463                 callback = self._recv_callback\n    464                 # self._recv_callback = None\n--> 465                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    466                 \n    467         # self.update_state()\n    468         \n    469 \n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    402         close our socket.\"\"\"\n    403         try:\n    404             # Use a NullContext to ensure that all StackContexts are run\n    405             # inside our blanket exception handler rather than outside.\n    406             with stack_context.NullContext():\n--> 407                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    408         except:\n    409             gen_log.error(\"Uncaught exception, closing connection.\",\n    410                           exc_info=True)\n    411             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    247         if self.control_stream:\n    248             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    249 \n    250         def make_dispatcher(stream):\n    251             def dispatcher(msg):\n--> 252                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    253             return dispatcher\n    254 \n    255         for s in self.shell_streams:\n    256             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"from sklearn.multiclass import OneVsRestClassifi...on: %f'%(metrics.precision_score(y_test, y_pred))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'session': 'FBDD4C6B97544B07916A781F436A5207', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'parent_header': {}})\n    208         else:\n    209             # ensure default_int_handler during handler call\n    210             sig = signal(SIGINT, default_int_handler)\n    211             self.log.debug(\"%s: %s\", msg_type, msg)\n    212             try:\n--> 213                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['FBDD4C6B97544B07916A781F436A5207']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"from sklearn.multiclass import OneVsRestClassifi...on: %f'%(metrics.precision_score(y_test, y_pred))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'session': 'FBDD4C6B97544B07916A781F436A5207', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'parent_header': {}}\n    214             except Exception:\n    215                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    216             finally:\n    217                 signal(SIGINT, sig)\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['FBDD4C6B97544B07916A781F436A5207'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"from sklearn.multiclass import OneVsRestClassifi...on: %f'%(metrics.precision_score(y_test, y_pred))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'session': 'FBDD4C6B97544B07916A781F436A5207', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'parent_header': {}})\n    357         if not silent:\n    358             self.execution_count += 1\n    359             self._publish_execute_input(code, parent, self.execution_count)\n    360 \n    361         reply_content = self.do_execute(code, silent, store_history,\n--> 362                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    363 \n    364         # Flush output before sending the reply.\n    365         sys.stdout.flush()\n    366         sys.stderr.flush()\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u\"from sklearn.multiclass import OneVsRestClassi...n: %f'%(metrics.precision_score(y_test, y_pred))\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    170 \n    171         reply_content = {}\n    172         # FIXME: the shell calls the exception handler itself.\n    173         shell._reply_content = None\n    174         try:\n--> 175             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u\"from sklearn.multiclass import OneVsRestClassi...n: %f'%(metrics.precision_score(y_test, y_pred))\"\n        store_history = True\n        silent = False\n    176         except:\n    177             status = u'error'\n    178             # FIXME: this code right now isn't being used yet by default,\n    179             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u\"from sklearn.multiclass import OneVsRestClassi...n: %f'%(metrics.precision_score(y_test, y_pred))\", store_history=True, silent=False, shell_futures=True)\n   2897                 self.displayhook.exec_result = result\n   2898 \n   2899                 # Execute the user code\n   2900                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2901                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2902                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2903 \n   2904                 # Reset this so later displayed values do not modify the\n   2905                 # ExecutionResult\n   2906                 self.displayhook.exec_result = None\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Print object>, <_ast.Print object>, <_ast.Print object>], cell_name='<ipython-input-59-644ea818905b>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3001 \n   3002         try:\n   3003             for i, node in enumerate(to_run_exec):\n   3004                 mod = ast.Module([node])\n   3005                 code = compiler(mod, cell_name, \"exec\")\n-> 3006                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x1096ea3b0, file \"<ipython-input-59-644ea818905b>\", line 4>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   3007                     return True\n   3008 \n   3009             for i, node in enumerate(to_run_interactive):\n   3010                 mod = ast.Interactive([node])\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x1096ea3b0, file \"<ipython-input-59-644ea818905b>\", line 4>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3061         outflag = 1  # happens in more places, so it's easier as default\n   3062         try:\n   3063             try:\n   3064                 self.hooks.pre_run_code_hook()\n   3065                 #rprint('Running code', repr(code_obj)) # dbg\n-> 3066                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x1096ea3b0, file \"<ipython-input-59-644ea818905b>\", line 4>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/no_dups.csv')\", u'len(master)', u'master.columns', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"data.to_csv('no_dups.csv')\", u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u'def word_count(text):\\n    out = text.translat...string.punctuation)\\n    return len(out.split())', u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u\"data['Recommendations'] = data['Recommendation...['Recommendations'].values\\nnp.percentile(x, 95)\", u\"data['Rec'] = data['Recommendations'].apply(la... 1 if x > 79 else 0)\\ndata['Rec'].value_counts()\", ...], 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {3: 88191, 4: Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Comment...idate', u'State', u'City'],\n      dtype='object'), 6: 88191, 8: 82882, 11: 82882, 13: 57579, 18: 84.0, 19: 0    54559\n1     3020\ndtype: int64, 20: Hilary     24982\nTrump      11679\nBush        68...59\nCarson      6372\nBiden       1248\ndtype: int64, 23: (array([ 54559.,      0.,      0.,      0.,      ...0.,      0.,      0.,      0.,      0.,   3020.]), array([ 0.        ,  0.00333333,  0.00666667,  0...99      ,  0.99333333,  0.99666667,  1.        ]), <a list of 300 Patch objects>), ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'SVC': <class 'sklearn.svm.classes.SVC'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/no_dups.csv')\", u'len(master)', u'master.columns', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"data.to_csv('no_dups.csv')\", u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u'def word_count(text):\\n    out = text.translat...string.punctuation)\\n    return len(out.split())', u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u\"data['Recommendations'] = data['Recommendation...['Recommendations'].values\\nnp.percentile(x, 95)\", u\"data['Rec'] = data['Recommendations'].apply(la... 1 if x > 79 else 0)\\ndata['Rec'].value_counts()\", ...], 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {3: 88191, 4: Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Comment...idate', u'State', u'City'],\n      dtype='object'), 6: 88191, 8: 82882, 11: 82882, 13: 57579, 18: 84.0, 19: 0    54559\n1     3020\ndtype: int64, 20: Hilary     24982\nTrump      11679\nBush        68...59\nCarson      6372\nBiden       1248\ndtype: int64, 23: (array([ 54559.,      0.,      0.,      0.,      ...0.,      0.,      0.,      0.,      0.,   3020.]), array([ 0.        ,  0.00333333,  0.00666667,  0...99      ,  0.99333333,  0.99666667,  1.        ]), <a list of 300 Patch objects>), ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'SVC': <class 'sklearn.svm.classes.SVC'>, ...}\n   3067             finally:\n   3068                 # Reset our crash handler in place\n   3069                 sys.excepthook = old_excepthook\n   3070         except SystemExit as e:\n\n...........................................................................\n/Users/datascientist/Project/<ipython-input-59-644ea818905b> in <module>()\n      1 \n      2 \n      3 from sklearn.multiclass import OneVsRestClassifier\n----> 4 base_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight = weights)\n      5 ovr = OneVsRestClassifier(estimator=base_clf, n_jobs=-1)\n      6 y_pred = ovr.fit(X_train, y_train).predict(X_test)\n      7 \n      8 print 'Accuracy: %f'%(metrics.accuracy_score(y_test, y_pred))\n      9 print 'Recall: %f'%(metrics.recall_score(y_test, y_pred))\n     10 print 'Precision: %f'%(metrics.precision_score(y_test, y_pred))\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/multiclass.py in fit(self=OneVsRestClassifier(estimator=RandomForestClassi...          warm_start=False),\n          n_jobs=-1), X=array([[  0.00000000e+00,   5.62500000e-02,   3....000000e+02,   0.00000000e+00,   4.70000000e+01]]), y=array([3, 5, 3, ..., 3, 2, 1]))\n    278         # of spawning threads.  See joblib issue #112.\n    279         self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)\n    280              (self.estimator, X, column,\n    281               classes=[\"not %s\" % self.label_binarizer_.classes_[i],\n    282                        self.label_binarizer_.classes_[i]])\n--> 283               for i, column in enumerate(columns))\n        columns = <generator object <genexpr>>\n    284 \n    285         return self\n    286 \n    287     def predict(self, X):\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object <genexpr>>)\n    661             if pre_dispatch == \"all\" or n_jobs == 1:\n    662                 # The iterable was consumed all at once by the above for loop.\n    663                 # No need to wait for async callbacks to trigger to\n    664                 # consumption.\n    665                 self._iterating = False\n--> 666             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    667             # Make sure that we get a last message telling us we are done\n    668             elapsed_time = time.time() - self._start_time\n    669             self._print('Done %3i out of %3i | elapsed: %s finished',\n    670                         (len(self._output),\n\n    ---------------------------------------------------------------------------\n    Sub-process traceback:\n    ---------------------------------------------------------------------------\n    IndexError                                         Mon Sep 28 10:31:28 2015\nPID: 42136          Python 2.7.10: /Users/datascientist/anaconda/bin/python\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc in _fit_binary(estimator=RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False), X=array([[  0.00000000e+00,   5.62500000e-02,   3....000000e+02,   0.00000000e+00,   4.70000000e+01]]), y=array([0, 0, 0, ..., 0, 0, 0]), classes=['not 0', 0])\n     69             warnings.warn(\"Label %s is present in all training examples.\" %\n     70                           str(classes[c]))\n     71         estimator = _ConstantPredictor().fit(X, unique_y)\n     72     else:\n     73         estimator = clone(estimator)\n---> 74         estimator.fit(X, y)\n     75     return estimator\n     76 \n     77 \n     78 def _predict_binary(estimator, X):\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc in fit(self=RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False), X=array([[  0.00000000e+00,   5.62499985e-02,   3.....00000000e+00,   4.70000000e+01]], dtype=float32), y=array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]]), sample_weight=None)\n    213             # [:, np.newaxis] that does not.\n    214             y = np.reshape(y, (-1, 1))\n    215 \n    216         self.n_outputs_ = y.shape[1]\n    217 \n--> 218         y, expanded_class_weight = self._validate_y_class_weight(y)\n    219 \n    220         if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n    221             y = np.ascontiguousarray(y, dtype=DOUBLE)\n    222 \n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc in _validate_y_class_weight(self=RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False), y=array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]]))\n    430                 if self.class_weight == 'subsample':\n    431                     class_weight = 'auto'\n    432                 else:\n    433                     class_weight = self.class_weight\n    434                 expanded_class_weight = compute_sample_weight(class_weight,\n--> 435                                                               y_original)\n        y.The = undefined\n        y = array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]])\n        self = RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False)\n        self.R = undefined\n    436 \n    437         return y, expanded_class_weight\n    438 \n    439     def predict(self, X):\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.pyc in compute_sample_weight(class_weight={0: 0.0, 1: 8.0904054263967272e-07, 2: 0.0, 3: 8.0904054263967272e-07, 4: 3.2361621705586909e-06, 5: 0.0}, y=array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]]), indices=None)\n    147 \n    148             classes_missing = set(classes_full) - set(classes_subsample)\n    149         else:\n    150             weight_k = compute_class_weight(class_weight_k,\n    151                                             classes_full,\n--> 152                                             y_full)\n    153 \n    154         weight_k = weight_k[np.searchsorted(classes_full, y_full)]\n    155 \n    156         if classes_missing:\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.pyc in compute_class_weight(class_weight={0: 0.0, 1: 8.0904054263967272e-07, 2: 0.0, 3: 8.0904054263967272e-07, 4: 3.2361621705586909e-06, 5: 0.0}, classes=array([0, 1]), y=array([0, 0, 0, ..., 0, 0, 0]))\n     55         if not isinstance(class_weight, dict):\n     56             raise ValueError(\"class_weight must be dict, 'auto', or None,\"\n     57                              \" got: %r\" % class_weight)\n     58         for c in class_weight:\n     59             i = np.searchsorted(classes, c)\n---> 60             if classes[i] != c:\n        y = array([0, 0, 0, ..., 0, 0, 0])\n     61                 raise ValueError(\"Class label %d not present.\" % c)\n     62             else:\n     63                 weight[i] = class_weight[c]\n     64 \n\nIndexError: index 2 is out of bounds for axis 0 with size 2\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJoblibIndexError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-644ea818905b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0movr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0movr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Accuracy: %f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    281\u001b[0m               classes=[\"not %s\" % self.label_binarizer_.classes_[i],\n\u001b[1;32m    282\u001b[0m                        self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 283\u001b[0;31m               for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m                         \u001b[0;31m# Convert this to a JoblibException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                         \u001b[0mexception_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mk_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibIndexError\u001b[0m: JoblibIndexError\n___________________________________________________________________________\nMultiprocessing exception:\n    ...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    157     pkg_name = mod_name.rpartition('.')[0]\n    158     main_globals = sys.modules[\"__main__\"].__dict__\n    159     if alter_argv:\n    160         sys.argv[0] = fname\n    161     return _run_code(code, main_globals, None,\n--> 162                      \"__main__\", fname, loader, pkg_name)\n        fname = '/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    163 \n    164 def run_module(mod_name, init_globals=None,\n    165                run_name=None, alter_sys=False):\n    166     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x1006fb0b0, file \"/Use...2.7/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/Users/datas...python2.7/site-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x1006fb0b0, file \"/Use...2.7/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/Users/datas...python2.7/site-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    587         \n    588         If a global instance already exists, this reinitializes and starts it\n    589         \"\"\"\n    590         app = cls.instance(**kwargs)\n    591         app.initialize(argv)\n--> 592         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    593 \n    594 #-----------------------------------------------------------------------------\n    595 # utility functions, for convenience\n    596 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    384     def start(self):\n    385         if self.poller is not None:\n    386             self.poller.start()\n    387         self.kernel.start()\n    388         try:\n--> 389             ioloop.IOLoop.instance().start()\n    390         except KeyboardInterrupt:\n    391             pass\n    392 \n    393 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    146             PollIOLoop.configure(ZMQIOLoop)\n    147         return PollIOLoop.instance()\n    148     \n    149     def start(self):\n    150         try:\n--> 151             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    152         except ZMQError as e:\n    153             if e.errno == ETERM:\n    154                 # quietly return on ETERM\n    155                 pass\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    835                 self._events.update(event_pairs)\n    836                 while self._events:\n    837                     fd, events = self._events.popitem()\n    838                     try:\n    839                         fd_obj, handler_func = self._handlers[fd]\n--> 840                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    841                     except (OSError, IOError) as e:\n    842                         if errno_from_exception(e) == errno.EPIPE:\n    843                             # Happens when the client closes the connection\n    844                             pass\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    428             # dispatch events:\n    429             if events & IOLoop.ERROR:\n    430                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    431                 return\n    432             if events & IOLoop.READ:\n--> 433                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    434                 if not self.socket:\n    435                     return\n    436             if events & IOLoop.WRITE:\n    437                 self._handle_send()\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    460                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    461         else:\n    462             if self._recv_callback:\n    463                 callback = self._recv_callback\n    464                 # self._recv_callback = None\n--> 465                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    466                 \n    467         # self.update_state()\n    468         \n    469 \n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    402         close our socket.\"\"\"\n    403         try:\n    404             # Use a NullContext to ensure that all StackContexts are run\n    405             # inside our blanket exception handler rather than outside.\n    406             with stack_context.NullContext():\n--> 407                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    408         except:\n    409             gen_log.error(\"Uncaught exception, closing connection.\",\n    410                           exc_info=True)\n    411             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    247         if self.control_stream:\n    248             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    249 \n    250         def make_dispatcher(stream):\n    251             def dispatcher(msg):\n--> 252                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    253             return dispatcher\n    254 \n    255         for s in self.shell_streams:\n    256             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"from sklearn.multiclass import OneVsRestClassifi...on: %f'%(metrics.precision_score(y_test, y_pred))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'session': 'FBDD4C6B97544B07916A781F436A5207', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'parent_header': {}})\n    208         else:\n    209             # ensure default_int_handler during handler call\n    210             sig = signal(SIGINT, default_int_handler)\n    211             self.log.debug(\"%s: %s\", msg_type, msg)\n    212             try:\n--> 213                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['FBDD4C6B97544B07916A781F436A5207']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"from sklearn.multiclass import OneVsRestClassifi...on: %f'%(metrics.precision_score(y_test, y_pred))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'session': 'FBDD4C6B97544B07916A781F436A5207', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'parent_header': {}}\n    214             except Exception:\n    215                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    216             finally:\n    217                 signal(SIGINT, sig)\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['FBDD4C6B97544B07916A781F436A5207'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"from sklearn.multiclass import OneVsRestClassifi...on: %f'%(metrics.precision_score(y_test, y_pred))\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'session': 'FBDD4C6B97544B07916A781F436A5207', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '536DFBEFBD4E49669A1367CC27504D28', 'msg_type': 'execute_request', 'parent_header': {}})\n    357         if not silent:\n    358             self.execution_count += 1\n    359             self._publish_execute_input(code, parent, self.execution_count)\n    360 \n    361         reply_content = self.do_execute(code, silent, store_history,\n--> 362                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    363 \n    364         # Flush output before sending the reply.\n    365         sys.stdout.flush()\n    366         sys.stderr.flush()\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u\"from sklearn.multiclass import OneVsRestClassi...n: %f'%(metrics.precision_score(y_test, y_pred))\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    170 \n    171         reply_content = {}\n    172         # FIXME: the shell calls the exception handler itself.\n    173         shell._reply_content = None\n    174         try:\n--> 175             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u\"from sklearn.multiclass import OneVsRestClassi...n: %f'%(metrics.precision_score(y_test, y_pred))\"\n        store_history = True\n        silent = False\n    176         except:\n    177             status = u'error'\n    178             # FIXME: this code right now isn't being used yet by default,\n    179             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u\"from sklearn.multiclass import OneVsRestClassi...n: %f'%(metrics.precision_score(y_test, y_pred))\", store_history=True, silent=False, shell_futures=True)\n   2897                 self.displayhook.exec_result = result\n   2898 \n   2899                 # Execute the user code\n   2900                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2901                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2902                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2903 \n   2904                 # Reset this so later displayed values do not modify the\n   2905                 # ExecutionResult\n   2906                 self.displayhook.exec_result = None\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Print object>, <_ast.Print object>, <_ast.Print object>], cell_name='<ipython-input-59-644ea818905b>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3001 \n   3002         try:\n   3003             for i, node in enumerate(to_run_exec):\n   3004                 mod = ast.Module([node])\n   3005                 code = compiler(mod, cell_name, \"exec\")\n-> 3006                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x1096ea3b0, file \"<ipython-input-59-644ea818905b>\", line 4>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   3007                     return True\n   3008 \n   3009             for i, node in enumerate(to_run_interactive):\n   3010                 mod = ast.Interactive([node])\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x1096ea3b0, file \"<ipython-input-59-644ea818905b>\", line 4>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3061         outflag = 1  # happens in more places, so it's easier as default\n   3062         try:\n   3063             try:\n   3064                 self.hooks.pre_run_code_hook()\n   3065                 #rprint('Running code', repr(code_obj)) # dbg\n-> 3066                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x1096ea3b0, file \"<ipython-input-59-644ea818905b>\", line 4>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/no_dups.csv')\", u'len(master)', u'master.columns', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"data.to_csv('no_dups.csv')\", u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u'def word_count(text):\\n    out = text.translat...string.punctuation)\\n    return len(out.split())', u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u\"data['Recommendations'] = data['Recommendation...['Recommendations'].values\\nnp.percentile(x, 95)\", u\"data['Rec'] = data['Recommendations'].apply(la... 1 if x > 79 else 0)\\ndata['Rec'].value_counts()\", ...], 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {3: 88191, 4: Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Comment...idate', u'State', u'City'],\n      dtype='object'), 6: 88191, 8: 82882, 11: 82882, 13: 57579, 18: 84.0, 19: 0    54559\n1     3020\ndtype: int64, 20: Hilary     24982\nTrump      11679\nBush        68...59\nCarson      6372\nBiden       1248\ndtype: int64, 23: (array([ 54559.,      0.,      0.,      0.,      ...0.,      0.,      0.,      0.,      0.,   3020.]), array([ 0.        ,  0.00333333,  0.00666667,  0...99      ,  0.99333333,  0.99666667,  1.        ]), <a list of 300 Patch objects>), ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'SVC': <class 'sklearn.svm.classes.SVC'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/no_dups.csv')\", u'len(master)', u'master.columns', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"import pandas as pd\\nimport numpy as np\\nimpor...ation\\nget_ipython().magic(u'matplotlib inline')\", u\"master = pd.read_csv('data/master.csv')\", u'len(master)', u\"dups = master.duplicated('Comment')\\ndata = master[-dups]\\n# data.to_csv('no_dups.csv')\", u'len(data)', u\"data.to_csv('no_dups.csv')\", u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u'def word_count(text):\\n    out = text.translat...string.punctuation)\\n    return len(out.split())', u\"data['word_count'] = data['Comment'].apply(lambda x: word_count(str(x)))\", u\"data['Recommendations'] = data['Recommendation...['Recommendations'].values\\nnp.percentile(x, 95)\", u\"data['Rec'] = data['Recommendations'].apply(la... 1 if x > 79 else 0)\\ndata['Rec'].value_counts()\", ...], 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {3: 88191, 4: Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Comment...idate', u'State', u'City'],\n      dtype='object'), 6: 88191, 8: 82882, 11: 82882, 13: 57579, 18: 84.0, 19: 0    54559\n1     3020\ndtype: int64, 20: Hilary     24982\nTrump      11679\nBush        68...59\nCarson      6372\nBiden       1248\ndtype: int64, 23: (array([ 54559.,      0.,      0.,      0.,      ...0.,      0.,      0.,      0.,      0.,   3020.]), array([ 0.        ,  0.00333333,  0.00666667,  0...99      ,  0.99333333,  0.99666667,  1.        ]), <a list of 300 Patch objects>), ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'SVC': <class 'sklearn.svm.classes.SVC'>, ...}\n   3067             finally:\n   3068                 # Reset our crash handler in place\n   3069                 sys.excepthook = old_excepthook\n   3070         except SystemExit as e:\n\n...........................................................................\n/Users/datascientist/Project/<ipython-input-59-644ea818905b> in <module>()\n      1 \n      2 \n      3 from sklearn.multiclass import OneVsRestClassifier\n----> 4 base_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight = weights)\n      5 ovr = OneVsRestClassifier(estimator=base_clf, n_jobs=-1)\n      6 y_pred = ovr.fit(X_train, y_train).predict(X_test)\n      7 \n      8 print 'Accuracy: %f'%(metrics.accuracy_score(y_test, y_pred))\n      9 print 'Recall: %f'%(metrics.recall_score(y_test, y_pred))\n     10 print 'Precision: %f'%(metrics.precision_score(y_test, y_pred))\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/multiclass.py in fit(self=OneVsRestClassifier(estimator=RandomForestClassi...          warm_start=False),\n          n_jobs=-1), X=array([[  0.00000000e+00,   5.62500000e-02,   3....000000e+02,   0.00000000e+00,   4.70000000e+01]]), y=array([3, 5, 3, ..., 3, 2, 1]))\n    278         # of spawning threads.  See joblib issue #112.\n    279         self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)\n    280              (self.estimator, X, column,\n    281               classes=[\"not %s\" % self.label_binarizer_.classes_[i],\n    282                        self.label_binarizer_.classes_[i]])\n--> 283               for i, column in enumerate(columns))\n        columns = <generator object <genexpr>>\n    284 \n    285         return self\n    286 \n    287     def predict(self, X):\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object <genexpr>>)\n    661             if pre_dispatch == \"all\" or n_jobs == 1:\n    662                 # The iterable was consumed all at once by the above for loop.\n    663                 # No need to wait for async callbacks to trigger to\n    664                 # consumption.\n    665                 self._iterating = False\n--> 666             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    667             # Make sure that we get a last message telling us we are done\n    668             elapsed_time = time.time() - self._start_time\n    669             self._print('Done %3i out of %3i | elapsed: %s finished',\n    670                         (len(self._output),\n\n    ---------------------------------------------------------------------------\n    Sub-process traceback:\n    ---------------------------------------------------------------------------\n    IndexError                                         Mon Sep 28 10:31:28 2015\nPID: 42136          Python 2.7.10: /Users/datascientist/anaconda/bin/python\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc in _fit_binary(estimator=RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False), X=array([[  0.00000000e+00,   5.62500000e-02,   3....000000e+02,   0.00000000e+00,   4.70000000e+01]]), y=array([0, 0, 0, ..., 0, 0, 0]), classes=['not 0', 0])\n     69             warnings.warn(\"Label %s is present in all training examples.\" %\n     70                           str(classes[c]))\n     71         estimator = _ConstantPredictor().fit(X, unique_y)\n     72     else:\n     73         estimator = clone(estimator)\n---> 74         estimator.fit(X, y)\n     75     return estimator\n     76 \n     77 \n     78 def _predict_binary(estimator, X):\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc in fit(self=RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False), X=array([[  0.00000000e+00,   5.62499985e-02,   3.....00000000e+00,   4.70000000e+01]], dtype=float32), y=array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]]), sample_weight=None)\n    213             # [:, np.newaxis] that does not.\n    214             y = np.reshape(y, (-1, 1))\n    215 \n    216         self.n_outputs_ = y.shape[1]\n    217 \n--> 218         y, expanded_class_weight = self._validate_y_class_weight(y)\n    219 \n    220         if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n    221             y = np.ascontiguousarray(y, dtype=DOUBLE)\n    222 \n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc in _validate_y_class_weight(self=RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False), y=array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]]))\n    430                 if self.class_weight == 'subsample':\n    431                     class_weight = 'auto'\n    432                 else:\n    433                     class_weight = self.class_weight\n    434                 expanded_class_weight = compute_sample_weight(class_weight,\n--> 435                                                               y_original)\n        y.The = undefined\n        y = array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]])\n        self = RandomForestClassifier(bootstrap=True,\n         ...te=None, verbose=0,\n            warm_start=False)\n        self.R = undefined\n    436 \n    437         return y, expanded_class_weight\n    438 \n    439     def predict(self, X):\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.pyc in compute_sample_weight(class_weight={0: 0.0, 1: 8.0904054263967272e-07, 2: 0.0, 3: 8.0904054263967272e-07, 4: 3.2361621705586909e-06, 5: 0.0}, y=array([[0],\n       [0],\n       [0],\n       ..., \n       [0],\n       [0],\n       [0]]), indices=None)\n    147 \n    148             classes_missing = set(classes_full) - set(classes_subsample)\n    149         else:\n    150             weight_k = compute_class_weight(class_weight_k,\n    151                                             classes_full,\n--> 152                                             y_full)\n    153 \n    154         weight_k = weight_k[np.searchsorted(classes_full, y_full)]\n    155 \n    156         if classes_missing:\n\n...........................................................................\n/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.pyc in compute_class_weight(class_weight={0: 0.0, 1: 8.0904054263967272e-07, 2: 0.0, 3: 8.0904054263967272e-07, 4: 3.2361621705586909e-06, 5: 0.0}, classes=array([0, 1]), y=array([0, 0, 0, ..., 0, 0, 0]))\n     55         if not isinstance(class_weight, dict):\n     56             raise ValueError(\"class_weight must be dict, 'auto', or None,\"\n     57                              \" got: %r\" % class_weight)\n     58         for c in class_weight:\n     59             i = np.searchsorted(classes, c)\n---> 60             if classes[i] != c:\n        y = array([0, 0, 0, ..., 0, 0, 0])\n     61                 raise ValueError(\"Class label %d not present.\" % c)\n     62             else:\n     63                 weight[i] = class_weight[c]\n     64 \n\nIndexError: index 2 is out of bounds for axis 0 with size 2\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "base_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight = weights)\n",
    "ovr = OneVsRestClassifier(estimator=base_clf, n_jobs=-1)\n",
    "y_pred = ovr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print 'Accuracy: %f'%(metrics.accuracy_score(y_test, y_pred))\n",
    "print 'Recall: %f'%(metrics.recall_score(y_test, y_pred))\n",
    "print 'Precision: %f'%(metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = reg['Candidate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88191"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_drop = data.drop_duplicates('Comment')\n",
    "len(data_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.027510801516621108,\n",
       " 1: 0.15075831055462482,\n",
       " 2: 0.14046380389736354,\n",
       " 3: 0.28143461775857509,\n",
       " 4: 0.14238162419539724,\n",
       " 5: 0.2574508420774182}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in comment.lower().split() if word not in stoplist]\n",
    "        for comment in content]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "          for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
    "model.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
