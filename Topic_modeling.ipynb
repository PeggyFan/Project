{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn.cluster\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import lstsq\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "bernie sanders support socialist media article win candidate senator campaign\n",
      "Topic #2:\n",
      "times news story nyt coverage new york ny article reporting\n",
      "Topic #3:\n",
      "trump donald fox mr fiorina debate gop bush media candidates\n",
      "Topic #4:\n",
      "biden joe run president vice man vp love legacy running\n",
      "Topic #5:\n",
      "clinton mrs ms email bush mr secretary server state campaign\n",
      "Topic #6:\n",
      "iran deal obama war nuclear israel president agreement world sanctions\n",
      "Topic #7:\n",
      "republican party vote candidate democratic candidates democrats republicans gop election\n",
      "Topic #8:\n",
      "people like don just time think know want really good\n",
      "Topic #9:\n",
      "hillary woman win obama email president think campaign democrats going\n",
      "Topic #10:\n",
      "college government money pay education students student tax tuition free\n"
     ]
    }
   ],
   "source": [
    "hilary = pd.read_csv('data/hilary_test.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(hilary['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=10).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br war times new america nation right world rand years\n",
      "Topic #2:\n",
      "obama president trade legacy deal republicans tpp congress mr did\n",
      "Topic #3:\n",
      "bernie sanders senator love president socialist thank campaign vermont win\n",
      "Topic #4:\n",
      "people just like don time government country think good need\n",
      "Topic #5:\n",
      "http com www href _blank target title org html blogspot\n",
      "Topic #6:\n",
      "party republican democratic democrats republicans left right candidate candidates election\n",
      "Topic #7:\n",
      "class middle working poor jobs american income americans economic politicians\n",
      "Topic #8:\n",
      "vote got ll don election voting primary voted republican know\n",
      "Topic #9:\n",
      "tax cuts taxes pay social security rich money income cut\n",
      "Topic #10:\n",
      "hillary clinton candidate bush mrs warren campaign ms woman issues\n"
     ]
    }
   ],
   "source": [
    "sanders = pd.read_csv('sanders_scores.csv')\n",
    "sanders[pd.isnull(sanders['Comment'])] = \"\"\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(sanders['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=10).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br com http www said did americans nation title target\n",
      "Topic #2:\n",
      "joe biden run man vp love vice president legacy need\n",
      "Topic #3:\n",
      "sanders bernie black african support voters media blow socialist biden\n",
      "Topic #4:\n",
      "people just like don think know really media want good\n",
      "Topic #5:\n",
      "trump donald brooks mr bush republican jeb gop like candidates\n",
      "Topic #6:\n",
      "clinton mrs bush email server state ms campaign biden secretary\n",
      "Topic #7:\n",
      "party republican vote democratic democrats candidate republicans election win candidates\n",
      "Topic #8:\n",
      "work hours time jobs working people workers pay labor americans\n",
      "Topic #9:\n",
      "president obama dowd vice ms mr black sharpton country maureen\n",
      "Topic #10:\n",
      "hillary woman better win think bernie candidate time women 2008\n"
     ]
    }
   ],
   "source": [
    "biden = pd.read_csv('biden_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(biden['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=10).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br right nation america tax course americans today new obama\n",
      "Topic #2:\n",
      "republican party republicans candidates candidate democrats democratic base voters vote\n",
      "Topic #3:\n",
      "bush jeb brother george safe president smarter kept family iraq\n",
      "Topic #4:\n",
      "trump donald media kelly supporters support campaign says news does\n",
      "Topic #5:\n",
      "people like just think don president really want say time\n",
      "Topic #6:\n",
      "com http www title href _blank target org 2015 https\n",
      "Topic #7:\n",
      "gop candidates candidate base debate women voters nomination fact run\n",
      "Topic #8:\n",
      "mr trump ramos bruni brooks blow column press thank ms\n",
      "Topic #9:\n",
      "sanders bernie hillary clinton biden candidate media times democrats democratic\n",
      "Topic #10:\n",
      "illegal immigration immigrants country american americans legal jobs laws law\n"
     ]
    }
   ],
   "source": [
    "trump = pd.read_csv('trump_scores.csv')\n",
    "pd.isnull(trump['Comment']).sum()\n",
    "#sanders[pd.isnull(sanders['Comment'])] = \"\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(trump['Comment'].values).toarray() \n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=10).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "         for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br obama right money hair world americans ll good wealth\n",
      "Topic #2:\n",
      "party republican republicans candidates candidate trump voters democratic democrats tea\n",
      "Topic #3:\n",
      "people country like illegal immigration don american just immigrants want\n",
      "Topic #4:\n",
      "trump donald president like think love man great just money\n",
      "Topic #5:\n",
      "gop trump candidates candidate run base voters nomination hate conservative\n",
      "Topic #6:\n",
      "com http www title href _blank target org 2015 html\n",
      "Topic #7:\n",
      "women kelly fox debate megyn news cnn woman did trump\n",
      "Topic #8:\n",
      "media sanders bernie dowd coverage times clinton news hillary time\n",
      "Topic #9:\n",
      "mr ramos trump press blow conference bruni did military question\n",
      "Topic #10:\n",
      "bush jeb president brother family like smart clinton george smarter\n"
     ]
    }
   ],
   "source": [
    "trump = pd.read_csv('trump1.csv')\n",
    "pd.isnull(trump['Comment']).sum()\n",
    "#sanders[pd.isnull(sanders['Comment'])] = \"\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(trump['Comment'].values).toarray() \n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=10).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "         for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "br rubio donald right said walker paul race did big\n",
      "Topic #2:\n",
      "jeb bush brother george did family clinton florida know house\n",
      "Topic #3:\n",
      "trump donald republican gop mr party media candidates debate like\n",
      "Topic #4:\n",
      "people like just don republican money time president republicans think\n",
      "Topic #5:\n",
      "com www http href _blank title target https 2015 bernie\n"
     ]
    }
   ],
   "source": [
    "bush = pd.read_csv('bush_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(bush['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=10).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1951c2df5657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 804\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 236\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/datascientist/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    117\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "carson = pd.read_csv('carson_scores.csv')\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 5000)\n",
    "                                 \n",
    "V = vectorizer.fit_transform(carson['Comment'].values).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "nmf = NMF(n_components=5).fit(V)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([features[i]\n",
    "                    for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
